{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextAnalyze:\n",
    "    def __init__(self, stop_words_file):\n",
    "        # load the stop_words file and initialize count\n",
    "        self.stop_words = self.load_stop_words(stop_words_file)\n",
    "        self.word_count = {}\n",
    "    \n",
    "    def load_stop_words(self, stop_words_file):\n",
    "        # open and load the stop words file\n",
    "        with open(stop_words_file, 'r') as file:\n",
    "            stop_words = file.read().splitlines()\n",
    "        return stop_words\n",
    "    \n",
    "    def process_text_file(self, file_path):\n",
    "        with open(file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                self.process_line(line)\n",
    "    \n",
    "    def process_line(self, line):\n",
    "        # strip and set the texts to lowercase\n",
    "        words = line.strip().split()\n",
    "        for word in words:\n",
    "            if word.isalpha():      #check if the word is alphabetic \n",
    "                if word.lower() not in self.stop_words:\n",
    "                    self.word_count[word] = self.word_count.get(word, 0) + 1\n",
    "\n",
    "    def get_top_words(self, k):\n",
    "        # sort and get the top k words\n",
    "        sorted_words = sorted(self.word_count.items(), key = lambda x: x[1], reverse=True)\n",
    "        return sorted_words[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "would,21795\n",
      "US,15810\n",
      "economic,14464\n",
      "countries,13136\n",
      "new,12460\n",
      "political,12229\n",
      "also,12018\n",
      "one,11708\n",
      "global,10935\n",
      "European,10824\n"
     ]
    }
   ],
   "source": [
    "# test \n",
    "analyzer = TextAnalyze(\"stop_words/NLTK's list of english stopwords\")\n",
    "analyzer.process_text_file('small_50MB_dataset.txt')\n",
    "top_words = analyzer.get_top_words(10)\n",
    "\n",
    "#print top words\n",
    "for word, count in top_words:\n",
    "    print(f'{word},{count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analyze performance \n",
    "import time\n",
    "import psutil\n",
    "\n",
    "def analyze_performance(filepath):\n",
    "    # Start measuring time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # get top k words \n",
    "    analyzer = TextAnalyze(\"stop_words/NLTK's list of english stopwords\")\n",
    "    analyzer.process_text_file(filepath)\n",
    "    top_words = analyzer.get_top_words(10)\n",
    "\n",
    "    #print top words\n",
    "    for word, count in top_words:\n",
    "        print(f'{word},{count}')\n",
    "\n",
    "    # Stop measuring time\n",
    "    end_time = time.time()\n",
    "    running_time = end_time - start_time\n",
    "    print(f\"Running Time: {running_time} seconds\")\n",
    "\n",
    "    # Get CPU utilization\n",
    "    cpu_utilization = psutil.cpu_percent()\n",
    "    print(f\"CPU Utilization: {cpu_utilization}%\")\n",
    "\n",
    "    # Get memory usage\n",
    "    memory_usage = psutil.Process().memory_info().rss / 1024 / 1024  # in MB\n",
    "    print(f\"Memory Usage: {memory_usage} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "would,21795\n",
      "US,15810\n",
      "economic,14464\n",
      "countries,13136\n",
      "new,12460\n",
      "political,12229\n",
      "also,12018\n",
      "one,11708\n",
      "global,10935\n",
      "European,10824\n",
      "Running Time: 7.834611892700195 seconds\n",
      "CPU Utilization: 11.5%\n",
      "Memory Usage: 130.60546875 MB\n"
     ]
    }
   ],
   "source": [
    "# Call the performance analysis function\n",
    "filepath = 'small_50MB_dataset.txt'\n",
    "analyze_performance(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "European,316713\n",
      "Mr,210158\n",
      "would,178550\n",
      "also,175427\n",
      "must,153717\n",
      "Commission,138001\n",
      "Member,119742\n",
      "like,107437\n",
      "Parliament,85550\n",
      "one,84992\n",
      "Running Time: 56.86176419258118 seconds\n",
      "CPU Utilization: 14.4%\n",
      "Memory Usage: 44.8125 MB\n"
     ]
    }
   ],
   "source": [
    "# 300MB\n",
    "filepath = 'data_300MB.txt'\n",
    "analyze_performance(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "said,1572125\n",
      "would,903267\n",
      "one,755766\n",
      "also,704422\n",
      "de,620092\n",
      "last,573309\n",
      "two,566546\n",
      "first,557474\n",
      "people,557166\n",
      "new,546573\n",
      "Running Time: 486.91321206092834 seconds\n",
      "CPU Utilization: 26.1%\n",
      "Memory Usage: 196.24609375 MB\n"
     ]
    }
   ],
   "source": [
    "# 2.5GB\n",
    "filepath = 'data_2.5GB.txt'\n",
    "analyze_performance(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "said,10397763\n",
      "would,5738120\n",
      "one,4664992\n",
      "also,4446636\n",
      "two,3535784\n",
      "last,3520997\n",
      "first,3458376\n",
      "people,3447034\n",
      "new,3377098\n",
      "could,3286270\n",
      "Running Time: 3063.7685120105743 seconds\n",
      "CPU Utilization: 33.4%\n",
      "Memory Usage: 283.8359375 MB\n"
     ]
    }
   ],
   "source": [
    "# 16GB\n",
    "filepath = 'data_16GB.txt'\n",
    "analyze_performance(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'would': 21795, 'US': 15810, 'economic': 14464, 'countries': 13136, 'new': 12460, 'political': 12229, 'also': 12018, 'one': 11708, 'global': 10935, 'European': 10824}\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import heapq # use heap to sort the hashmap\n",
    "\n",
    "class WordCounterThread(threading.Thread):\n",
    "    def __init__(self, partition, stop_words):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.partition = partition\n",
    "        self.word_count = {}\n",
    "        self.stop_words = stop_words\n",
    "\n",
    "    def run(self):\n",
    "        # Process the partition and count the words\n",
    "        for line in self.partition:\n",
    "            words = line.strip().split()\n",
    "            for word in words:\n",
    "                if word.isalpha():\n",
    "                    if word.lower() not in self.stop_words:\n",
    "                        self.word_count[word] = self.word_count.get(word, 0) + 1\n",
    "\n",
    "    def get_word_count(self):\n",
    "        return self.word_count\n",
    "\n",
    "def create_partitions(file_path, num_partitions):\n",
    "    partitions = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        total_lines = len(lines)\n",
    "        lines_per_partition = total_lines // num_partitions\n",
    "\n",
    "        start = 0\n",
    "        for i in range(num_partitions - 1):\n",
    "            end = start + lines_per_partition\n",
    "            partition = lines[start:end]\n",
    "            partitions.append(partition)\n",
    "            start = end\n",
    "\n",
    "        # Last partition may have remaining lines\n",
    "        last_partition = lines[start:]\n",
    "        partitions.append(last_partition)\n",
    "\n",
    "    return partitions\n",
    "\n",
    "def load_stop_words(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        stop_words = set(file.read().split())\n",
    "    return stop_words\n",
    "\n",
    "def count_words_parallel(file_path, stop_words_file, num_threads, k):\n",
    "    partitions = create_partitions(file_path, num_threads)\n",
    "    stop_words = load_stop_words(stop_words_file)\n",
    "\n",
    "    # Create and start the threads\n",
    "    threads = []\n",
    "    for i in range(num_threads):\n",
    "        thread = WordCounterThread(partitions[i], stop_words)\n",
    "        threads.append(thread)\n",
    "        thread.start()\n",
    "\n",
    "    # Wait for all threads to finish\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "\n",
    "    # Merge the word counts from each thread\n",
    "    merged_word_count = {}\n",
    "    for thread in threads:\n",
    "        word_count = thread.get_word_count()\n",
    "        for word, count in word_count.items():\n",
    "            merged_word_count[word] = merged_word_count.get(word, 0) + count\n",
    "\n",
    "    # Get the top k words based on counts\n",
    "    top_words = heapq.nlargest(k, merged_word_count, key=merged_word_count.get)\n",
    "    top_word_count = {word: merged_word_count[word] for word in top_words}\n",
    "\n",
    "    return top_word_count\n",
    "\n",
    "# Usage example\n",
    "file_path = 'small_50MB_dataset.txt'\n",
    "stop_words_file = \"stop_words/NLTK's list of english stopwords\"\n",
    "num_threads = 4\n",
    "k = 10\n",
    "top_word_count = count_words_parallel(file_path, stop_words_file, num_threads, k)\n",
    "print(top_word_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analyze performance \n",
    "\n",
    "def analyze_performance_2(filepath, stop_words_file, num_threads, k):\n",
    "    # Start measuring time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # get top k words \n",
    "    top_word_count = count_words_parallel(file_path, stop_words_file, num_threads, k)\n",
    "    print(top_word_count)\n",
    "\n",
    "    # Stop measuring time\n",
    "    end_time = time.time()\n",
    "    running_time = end_time - start_time\n",
    "    print(f\"Running Time: {running_time} seconds\")\n",
    "\n",
    "    # Get CPU utilization\n",
    "    cpu_utilization = psutil.cpu_percent()\n",
    "    print(f\"CPU Utilization: {cpu_utilization}%\")\n",
    "\n",
    "    # Get memory usage\n",
    "    memory_usage = psutil.Process().memory_info().rss / 1024 / 1024  # in MB\n",
    "    print(f\"Memory Usage: {memory_usage} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'would': 21795, 'US': 15810, 'economic': 14464, 'countries': 13136, 'new': 12460, 'political': 12229, 'also': 12018, 'one': 11708, 'global': 10935, 'European': 10824}\n",
      "Running Time: 14.21212387084961 seconds\n",
      "CPU Utilization: 31.4%\n",
      "Memory Usage: 2358.015625 MB\n"
     ]
    }
   ],
   "source": [
    "# 50 MB 4 threads\n",
    "file_path = 'small_50MB_dataset.txt'\n",
    "analyze_performance_2(file_path,stop_words_file, 4, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'European': 316713, 'Mr': 210158, 'would': 178550, 'also': 175427, 'must': 153717, 'Commission': 138001, 'Member': 119742, 'like': 107437, 'Parliament': 85550, 'one': 84992}\n",
      "Running Time: 16.15682315826416 seconds\n",
      "CPU Utilization: 27.5%\n",
      "Memory Usage: 1871.66015625 MB\n"
     ]
    }
   ],
   "source": [
    "# 300 MB 4 threads\n",
    "file_path = 'data_300MB.txt'\n",
    "analyze_performance_2(file_path,stop_words_file, 4, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'European': 316713, 'Mr': 210158, 'would': 178550, 'also': 175427, 'must': 153717, 'Commission': 138001, 'Member': 119742, 'like': 107437, 'Parliament': 85550, 'one': 84992}\n",
      "Running Time: 15.069303035736084 seconds\n",
      "CPU Utilization: 26.3%\n",
      "Memory Usage: 1857.77734375 MB\n"
     ]
    }
   ],
   "source": [
    "# 300 MB 8 threads\n",
    "file_path = 'data_300MB.txt'\n",
    "analyze_performance_2(file_path,stop_words_file, 8, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'said': 1572125, 'would': 903267, 'one': 755766, 'also': 704422, 'de': 620092, 'last': 573309, 'two': 566546, 'first': 557474, 'people': 557166, 'new': 546573}\n",
      "Running Time: 154.55087280273438 seconds\n",
      "CPU Utilization: 34.9%\n",
      "Memory Usage: 2695.95703125 MB\n"
     ]
    }
   ],
   "source": [
    "# 2.5 GB 4 threads\n",
    "file_path = 'data_2.5GB.txt'\n",
    "analyze_performance_2(file_path,stop_words_file, 4, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'said': 1572125, 'would': 903267, 'one': 755766, 'also': 704422, 'de': 620092, 'last': 573309, 'two': 566546, 'first': 557474, 'people': 557166, 'new': 546573}\n",
      "Running Time: 150.8342468738556 seconds\n",
      "CPU Utilization: 32.4%\n",
      "Memory Usage: 2580.6484375 MB\n"
     ]
    }
   ],
   "source": [
    "# 2.5 GB 8 threads\n",
    "file_path = 'data_2.5GB.txt'\n",
    "analyze_performance_2(file_path,stop_words_file, 8, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'said': 10397763, 'would': 5738120, 'one': 4664992, 'also': 4446636, 'two': 3535784, 'last': 3520997, 'first': 3458376, 'people': 3447034, 'new': 3377098, 'could': 3286270}\n",
      "Running Time: 1302.087240934372 seconds\n",
      "CPU Utilization: 28.8%\n",
      "Memory Usage: 2544.13671875 MB\n"
     ]
    }
   ],
   "source": [
    "# 16 GB 4 threads \n",
    "file_path = 'data_16GB.txt'\n",
    "analyze_performance_2(file_path,stop_words_file, 4, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'said': 10397763, 'would': 5738120, 'one': 4664992, 'also': 4446636, 'two': 3535784, 'last': 3520997, 'first': 3458376, 'people': 3447034, 'new': 3377098, 'could': 3286270}\n",
      "Running Time: 1433.357969045639 seconds\n",
      "CPU Utilization: 38.8%\n",
      "Memory Usage: 3808.78515625 MB\n"
     ]
    }
   ],
   "source": [
    "# 16 GB 8 threads \n",
    "file_path = 'data_16GB.txt'\n",
    "analyze_performance_2(file_path,stop_words_file, 8, 10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from above, we can see that using multi threading and partition significantly increase the speed of reading large size of data. Using more threads sometimes result in faster reading speed but the cpu and memory usage are much higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'would': 21795, 'US': 15810, 'economic': 14464, 'countries': 13136, 'new': 12460, 'political': 12229, 'also': 12018, 'one': 11708, 'global': 10935, 'European': 10824}\n"
     ]
    }
   ],
   "source": [
    "# using concurrent hashmap\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def count_words_concurrent(file_path, stop_words_file, num_threads, k):\n",
    "    partitions = create_partitions(file_path, num_threads)\n",
    "    stop_words = load_stop_words(stop_words_file)\n",
    "\n",
    "    def count_words(partition):\n",
    "        word_count = {}\n",
    "        for line in partition:\n",
    "            words = line.strip().split()\n",
    "            for word in words:\n",
    "                if word.isalpha():\n",
    "                    if word.lower() not in stop_words:\n",
    "                        word_count[word] = word_count.get(word, 0) + 1\n",
    "        return word_count\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        word_count_futures = executor.map(count_words, partitions)\n",
    "\n",
    "    merged_word_count = {}\n",
    "    for word_count in word_count_futures:\n",
    "        for word, count in word_count.items():\n",
    "            merged_word_count[word] = merged_word_count.get(word, 0) + count\n",
    "\n",
    "    # Get the top k words based on counts\n",
    "    top_words = heapq.nlargest(k, merged_word_count, key=merged_word_count.get)\n",
    "    top_word_count = {word: merged_word_count[word] for word in top_words}\n",
    "\n",
    "    return top_word_count\n",
    "\n",
    "# Usage example\n",
    "file_path = 'small_50MB_dataset.txt'\n",
    "stop_words_file = \"stop_words/NLTK's list of english stopwords\"\n",
    "num_threads = 4\n",
    "k = 10\n",
    "top_word_count = count_words_concurrent(file_path, stop_words_file, num_threads, k)\n",
    "print(top_word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analyze performance \n",
    "\n",
    "def analyze_performance_3(filepath, stop_words_file, num_threads, k):\n",
    "    # Start measuring time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # get top k words \n",
    "    top_word_count = count_words_concurrent(file_path, stop_words_file, num_threads, k)\n",
    "    print(top_word_count)\n",
    "\n",
    "    # Stop measuring time\n",
    "    end_time = time.time()\n",
    "    running_time = end_time - start_time\n",
    "    print(f\"Running Time: {running_time} seconds\")\n",
    "\n",
    "    # Get CPU utilization\n",
    "    cpu_utilization = psutil.cpu_percent()\n",
    "    print(f\"CPU Utilization: {cpu_utilization}%\")\n",
    "\n",
    "    # Get memory usage\n",
    "    memory_usage = psutil.Process().memory_info().rss / 1024 / 1024  # in MB\n",
    "    print(f\"Memory Usage: {memory_usage} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'European': 316713, 'Mr': 210158, 'would': 178550, 'also': 175427, 'must': 153717, 'Commission': 138001, 'Member': 119742, 'like': 107437, 'Parliament': 85550, 'one': 84992}\n",
      "Running Time: 15.011220932006836 seconds\n",
      "CPU Utilization: 39.3%\n",
      "Memory Usage: 654.55859375 MB\n"
     ]
    }
   ],
   "source": [
    "# 300 MB 4 threads\n",
    "file_path = 'data_300MB.txt'\n",
    "analyze_performance_3(file_path,stop_words_file, 4, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'European': 316713, 'Mr': 210158, 'would': 178550, 'also': 175427, 'must': 153717, 'Commission': 138001, 'Member': 119742, 'like': 107437, 'Parliament': 85550, 'one': 84992}\n",
      "Running Time: 13.844422817230225 seconds\n",
      "CPU Utilization: 36.9%\n",
      "Memory Usage: 630.23046875 MB\n"
     ]
    }
   ],
   "source": [
    "# 300 MB 8 threads\n",
    "file_path = 'data_300MB.txt'\n",
    "analyze_performance_3(file_path,stop_words_file, 4, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
